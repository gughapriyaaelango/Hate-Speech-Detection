{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOXv9UO9F-f0",
        "outputId": "a4dc5e65-eb20-42ae-c87d-8509a515f627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "df = pd.read_csv('/content/drive/Shareddrives/Text mining Project/PROJECT/TWEET 2.csv')"
      ],
      "metadata": {
        "id": "Ek5BkhFaKT0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip glove.twitter.27B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdbNzJxsP6ZD",
        "outputId": "f4b879ea-6b22-4872-f300-b639655907f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-05 07:01:01--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
            "--2023-12-05 07:01:02--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2023-12-05 07:01:02--  https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  5.01MB/s    in 4m 44s  \n",
            "\n",
            "2023-12-05 07:05:47 (5.10 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n",
            "Archive:  glove.twitter.27B.zip\n",
            "  inflating: glove.twitter.27B.25d.txt  \n",
            "  inflating: glove.twitter.27B.50d.txt  \n",
            "  inflating: glove.twitter.27B.100d.txt  \n",
            "  inflating: glove.twitter.27B.200d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "glove_model = KeyedVectors.load_word2vec_format('glove.twitter.27B.200d.txt', binary=False, no_header=True)\n",
        "\n",
        "\n",
        "# Function to convert a sentence to a vector using GloVe word vectors\n",
        "def sentence_to_vector(sentence, model):\n",
        "    words = sentence.split()\n",
        "    vector = sum([model[word] for word in words if word in model])\n",
        "    return vector / len(words) if len(words) > 0 else np.zeros(200)  # Adjust dimension based on the loaded model\n",
        "\n",
        "# Apply the function to the 'text' column in your DataFrame\n",
        "df['embedding'] = df['text'].apply(lambda x: sentence_to_vector(x, glove_model))\n"
      ],
      "metadata": {
        "id": "oEbL9fvoKZ8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['embedding']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65d9azvDKZ-0",
        "outputId": "1ec27b18-edbb-4a0b-dcf7-def8ce9b425d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [-0.012313548, 0.029266087, 0.008681902, 0.299...\n",
              "1        [-0.052466247, -0.0930375, 0.1549125, 0.065154...\n",
              "2        [0.02908667, -0.16832334, 0.22968334, 0.175442...\n",
              "3        [-0.040377542, 0.15304908, 0.09444019, 0.23846...\n",
              "4        [0.21581136, 0.03897329, 0.17565462, -0.003136...\n",
              "                               ...                        \n",
              "40618    [0.026347542, 0.15136111, 0.021407995, -0.1230...\n",
              "40619    [0.1294142, 0.043254435, 0.00953022, -0.117486...\n",
              "40620    [-0.029319227, -0.035034575, 0.24909759, 0.282...\n",
              "40621    [-0.010016665, -0.17406034, -0.029616654, 0.34...\n",
              "40622    [-0.08816388, 0.073139995, 0.0424425, 0.224486...\n",
              "Name: embedding, Length: 40623, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "\n",
        "# Load Twitter data\n",
        "data = pd.read_csv('/content/drive/Shareddrives/Text mining Project/PROJECT/TWEET 2.csv')\n",
        "\n",
        "# Preprocess data\n",
        "X = data['text'].values\n",
        "y = LabelEncoder().fit_transform(data['label'])\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_seq = tokenizer.texts_to_sequences(X)\n",
        "X_padded = pad_sequences(X_seq, maxlen=max_len)\n",
        "\n",
        "# Load pre-trained GloVe embeddings\n",
        "embedding_dim = 100\n",
        "embedding_index = {}\n",
        "with open('glove.twitter.27B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "# Create an embedding matrix\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Ensure the dimensions match\n",
        "            embedding_matrix[i] = embedding_vector[:embedding_dim]\n",
        "# Build DCNN model with GloVe embeddings\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
        "model.add(Conv1D(300, 1, activation='relu'))\n",
        "model.add(Conv1D(300, 2, activation='relu'))\n",
        "model.add(Conv1D(300, 3, activation='relu'))\n",
        "model.add(Conv1D(300, 4, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model architecture\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_padded, y, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = (model.predict(X_padded) > 0.5).astype(int)\n",
        "print(classification_report(y, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STtQ1f2v0j9E",
        "outputId": "3b6f1aa4-42c3-4088-9071-e234d90163f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 100, 300)          30300     \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 99, 300)           180300    \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 97, 300)           270300    \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 94, 300)           360300    \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 300)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 300)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                19264     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1860529 (7.10 MB)\n",
            "Trainable params: 860529 (3.28 MB)\n",
            "Non-trainable params: 1000000 (3.81 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1016/1016 [==============================] - 377s 369ms/step - loss: 0.5941 - accuracy: 0.6765 - val_loss: 0.5703 - val_accuracy: 0.7103\n",
            "Epoch 2/5\n",
            "1016/1016 [==============================] - 387s 381ms/step - loss: 0.5070 - accuracy: 0.7495 - val_loss: 0.5516 - val_accuracy: 0.7344\n",
            "Epoch 3/5\n",
            "1016/1016 [==============================] - 364s 358ms/step - loss: 0.4656 - accuracy: 0.7697 - val_loss: 0.5327 - val_accuracy: 0.7375\n",
            "Epoch 4/5\n",
            "1016/1016 [==============================] - 368s 362ms/step - loss: 0.4296 - accuracy: 0.7915 - val_loss: 0.5425 - val_accuracy: 0.7381\n",
            "Epoch 5/5\n",
            "1016/1016 [==============================] - 382s 376ms/step - loss: 0.3971 - accuracy: 0.8103 - val_loss: 0.5277 - val_accuracy: 0.7372\n",
            "1270/1270 [==============================] - 129s 101ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84     22124\n",
            "           1       0.83      0.75      0.79     18499\n",
            "\n",
            "    accuracy                           0.82     40623\n",
            "   macro avg       0.82      0.81      0.81     40623\n",
            "weighted avg       0.82      0.82      0.82     40623\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y, y_pred))\n",
        "\n",
        "# Additional metrics\n",
        "accuracy = accuracy_score(y, y_pred)\n",
        "conf_matrix = confusion_matrix(y, y_pred)\n",
        "\n",
        "# Print additional metrics\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate and print sensitivity and specificity\n",
        "true_negatives = conf_matrix[0, 0]\n",
        "false_negatives = conf_matrix[1, 0]\n",
        "true_positives = conf_matrix[1, 1]\n",
        "false_positives = conf_matrix[0, 1]\n",
        "\n",
        "sensitivity = true_positives / (true_positives + false_negatives)\n",
        "specificity = true_negatives / (true_negatives + false_positives)\n",
        "\n",
        "print(f'Sensitivity: {sensitivity}')\n",
        "print(f'Specificity: {specificity}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qFFkZ7R0sB5",
        "outputId": "e90227ce-076e-451f-fbe5-7f4b9d482c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84     22124\n",
            "           1       0.83      0.75      0.79     18499\n",
            "\n",
            "    accuracy                           0.82     40623\n",
            "   macro avg       0.82      0.81      0.81     40623\n",
            "weighted avg       0.82      0.82      0.82     40623\n",
            "\n",
            "Accuracy: 0.8167540555842749\n",
            "Confusion Matrix:\n",
            "[[19218  2906]\n",
            " [ 4538 13961]]\n",
            "Sensitivity: 0.7546894426725769\n",
            "Specificity: 0.8686494304827337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_tweets = [\"I love spreading kindness and positivity! #SpreadLove\",\n",
        "    \"This is the worst thing ever! I can't believe people are so hateful. #Angry\",\n",
        "    \"Why are muslims always fighting with others? Dont they have anything better to do?\",\n",
        "    \"I can't stand women who act like they know it all.\",\n",
        "    \"This is unacceptable! How can anyone say such hurtful things? #Angry\",\n",
        "    \"Gay teachers should not be allowed near children\",\n",
        "    \"Why are immigrants stealing all our jobs and money\",\n",
        "    \"It's important to address hate speech and work towards building a more inclusive society. #SocialJustice\"]\n",
        "\n",
        "# Tokenize and pad sequences for new tweets\n",
        "new_sequences = tokenizer.texts_to_sequences(new_tweets)\n",
        "new_padded = pad_sequences(new_sequences, maxlen=max_len)\n",
        "\n",
        "# Predict sentiment for new tweets\n",
        "new_predictions = (model.predict(new_padded) > 0.5).astype(int)\n",
        "\n",
        "# Print the results\n",
        "for tweet, prediction in zip(new_tweets, new_predictions):\n",
        "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "    print(f'Tweet: \"{tweet}\" - Sentiment: {sentiment}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py619tHD4Wj3",
        "outputId": "27921619-ef3c-445f-8b15-b54bd404b521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 81ms/step\n",
            "Tweet: \"I love spreading kindness and positivity! #SpreadLove\" - Sentiment: Positive\n",
            "Tweet: \"This is the worst thing ever! I can't believe people are so hateful. #Angry\" - Sentiment: Positive\n",
            "Tweet: \"Why are muslims always fighting with others? Dont they have anything better to do?\" - Sentiment: Negative\n",
            "Tweet: \"I can't stand women who act like they know it all.\" - Sentiment: Negative\n",
            "Tweet: \"This is unacceptable! How can anyone say such hurtful things? #Angry\" - Sentiment: Positive\n",
            "Tweet: \"Gay teachers should not be allowed near children\" - Sentiment: Negative\n",
            "Tweet: \"Why are immigrants stealing all our jobs and money\" - Sentiment: Negative\n",
            "Tweet: \"It's important to address hate speech and work towards building a more inclusive society. #SocialJustice\" - Sentiment: Positive\n"
          ]
        }
      ]
    }
  ]
}